input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics           => ["orders"]
    group_id         => "logstash-orders-hdfs-v3"   # 用新的 group，避免用旧 offset
    auto_offset_reset => "earliest"
    codec            => json
  }
}

filter {
  date {
    match          => ["order_time", "ISO8601"]
    target         => "@timestamp"
    tag_on_failure => []
  }

  mutate {
    add_field => {
      "csv_line" => "%{order_id},%{user_id},%{product_id},%{category},%{price},%{quantity},%{order_time},%{status}"
    }
  }
}

output {
  # ====== 写 Elasticsearch（实时） ======
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "orders-%{+YYYY.MM.dd}"
  }

  # ====== 写 HDFS（真正让 Hive 能读的部分）======
  webhdfs {
    host => "namenode"
    port => 9870
    user => "root"

    # 【关键修改 1】文件名去掉了秒和 order_id
    # 现在 Logstash 会把同一小时内的数据都写进同一个文件里（例如 orders-2025-12-24-15.log）
    path => "/data/orders/orders-%{+YYYY-MM-dd-HH}.log"

    # 【关键修改 2】开启“攒批”模式
    # flush_size: 内存里攒够 500 条数据才去写一次 HDFS（减少网络请求）
    flush_size => 500
    
    # idle_flush_time: 如果数据少，攒不够 500 条怎么办？
    # 只要等待超过 30 秒，不管有多少条都强制写入（保证实时性不至于太差）
    idle_flush_time => 30

    codec => line {
      format => "%{csv_line}"
    }
  }
}
